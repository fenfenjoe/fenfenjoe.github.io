import{_ as s,c as e,a as n,o as l}from"./app-iMoEB5u2.js";const i={};function p(r,a){return l(),e("div",null,a[0]||(a[0]=[n(`<h1 id="hadoop学习笔记" tabindex="-1"><a class="header-anchor" href="#hadoop学习笔记"><span>Hadoop学习笔记</span></a></h1><h3 id="参考" tabindex="-1"><a class="header-anchor" href="#参考"><span>参考</span></a></h3><p>《Hadoop权威指南》</p><h3 id="简介" tabindex="-1"><a class="header-anchor" href="#简介"><span>简介</span></a></h3><p>底层语言：Java<br> Apache官网：https://hadoop.apache.org/</p><p>特点：</p><ul><li>数据本地化：在计算节点上存储数据，实现数据的快速访问；（区别于“高性能计算”和“网格计算”）</li><li>无共享（shared-nothing）框架：各台机器上的任务彼此独立，而且MapReduce提供失败检测</li></ul><p>设计目标：</p><ol><li>为只需几分钟或几小时可完成的作业提供服务；（意思是最好一天内？）</li><li>运行于一个内部有高速网络连接的数据中心；（意思是不要跨区域的机器）</li><li>数据中心内的计算机都是可靠、定制的硬件；（意思是尽量自己部署的机器）</li></ol><p>版本兼容性：<br> 大版本之间是允许破坏API兼容性的（即1.x的版本和2.0版本的API会不兼容）；</p><p>两大组件：</p><ul><li>HDFS</li><li>MapReduce</li><li>Yarn（2.0）</li></ul><h3 id="hdfs" tabindex="-1"><a class="header-anchor" href="#hdfs"><span>HDFS</span></a></h3><p>HDFS是Hadoop体系内的一个分布式文件系统框架。</p><p>用户在Linux上安装好HDFS后，启动HDFS的客户端（client），通过HDFS提供的命令行操作，便可在该分布式文件系统中做增加、删除、查询文件等的操作。</p><h4 id="优点" tabindex="-1"><a class="header-anchor" href="#优点"><span>优点</span></a></h4><ul><li>高容错：数据冗余、多副本，数据丢失可恢复</li><li>高可用：NameNode HA、安全模式</li><li>高扩展：支持10000个节点规模</li><li>海量数据存储：典型文件大小GB~TB，PB以上数据规模</li><li>构建成本低：可部署到价格低廉的服务器上</li></ul><h4 id="缺点" tabindex="-1"><a class="header-anchor" href="#缺点"><span>缺点</span></a></h4><ul><li>不支持低延迟访问</li><li>不适合大量小文件存储</li><li>不支持文件的并发写入（可并发读）</li><li>不支持文件的随机修改</li></ul><h4 id="架构" tabindex="-1"><a class="header-anchor" href="#架构"><span>架构</span></a></h4><p>HDFS集群是由一个NameNode节点（也可以多一台做热备）和多个DataNode节点组成。<br> 简单的说，DataNode负责存储文件，NameNode负责管理DataNode节点。</p><h5 id="namenode-管理节点" tabindex="-1"><a class="header-anchor" href="#namenode-管理节点"><span>NameNode（管理节点）</span></a></h5><p>相当于一个管理进程，负责内容如下：</p><ul><li>负责集群中DataNode的元数据维护、状态同步；</li><li>client端发送过来的请求后，NameNode负责将DataNode的信息返回给client端（相当于路由的功能），然后client端直接与DataNode联系</li><li>管理Block副本策略（什么是Block？见DataNode的说明）：默认是3个副本，即默认一个块在3个DataNode节点中都有保存副本。</li></ul><p>NameNode还提供<strong>热备功能</strong>：</p><ul><li>Active NameNode：活动的Master管理节点（唯一）</li><li>Standby NameNode：热备管理节点，Master挂掉后自动顶上，升级为Master；</li></ul><blockquote><p>Hadoop3.0后才支持有多个Standby NameNode</p></blockquote><p>NameNode维护的元数据信息都保存在内存中。这些元数据信息是怎么持久化的呢？</p><blockquote><p>通过周期性同步的方式，将内存中的元数据信息以fsimage和edits的形式保存到硬盘。<br> 其中，fsimage相当于内存快照，即某段时间内存里的数据；<br> edits是操作日志，只会记录写操作；</p></blockquote><h6 id="元数据文件详解" tabindex="-1"><a class="header-anchor" href="#元数据文件详解"><span>元数据文件详解</span></a></h6><p><strong>fsimage（元数据检查点镜像文件）</strong><br> NameNode运行期间内存的元数据信息，会在执行<strong>元数据检查点（Checkpoint）</strong> 后，被持久化成一个fsimage，保存到文件系统中。元数据信息包括：<br> 目录下有哪些文件、文件名、副本数、文件由哪些Block组成等。</p><p><strong>edits（操作日志文件）</strong><br> 保存了<strong>元数据检查点</strong>后的所有文件更新操作。</p><blockquote><p>对<strong>元数据检查点（Checkpoint）</strong> 的理解：<br> 实质是一次将fsimage与edits合并的，生成一个新的fsimage的操作。<br> 因为对fsimage执行edits中的操作后，就可以得到最新的数据。因此可通过Checkpoint还原当时内存里的数据情况。<br> 当集群规模过大，为了减轻NameNode的负担，可将Checkpoint交由<strong>Secondary NameNode</strong> 或者<strong>Standby NameNode</strong> 负责</p></blockquote><h5 id="datanode-存储节点" tabindex="-1"><a class="header-anchor" href="#datanode-存储节点"><span>DataNode（存储节点）</span></a></h5><p>HDFS将需要存储的大文件切割成块（Block），存储在DataNode进程所在的节点中。一般一个节点部署一个DataNode进程。<br> 不同版本，块切割的大小不一样：</p><ul><li>Hadoop1.0 1Block=64MB</li><li>Hadoop2.0 1Block=128MB</li></ul><p>DataNode的特点：</p><ul><li>可大规模扩展</li><li>保存Block</li><li>通过心跳机制（默认3s）向NameNode汇报自己的状态和Block信息</li><li>执行client的读写请求</li></ul><h4 id="hdfs命令行操作" tabindex="-1"><a class="header-anchor" href="#hdfs命令行操作"><span>HDFS命令行操作</span></a></h4><p><strong>启动集群</strong></p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">/usr/local/hadoop/sbin/start-dfs.sh</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p><strong>查看集群中节点信息</strong></p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line">hdfs dfsadmin <span class="token parameter variable">-report</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p><strong>文件操作（hdfs dfs）</strong></p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code class="language-bash"><span class="line"><span class="token comment">#跟hadoop fs命令一样</span></span>
<span class="line">hdfs dfs </span>
<span class="line">    <span class="token parameter variable">-mkdir</span> <span class="token operator">&lt;</span>目录名<span class="token operator">&gt;</span> <span class="token comment">#创建目录</span></span>
<span class="line">    <span class="token parameter variable">-touchz</span> <span class="token operator">&lt;</span>文件名<span class="token operator">&gt;</span> <span class="token comment">#创建文件</span></span>
<span class="line">    <span class="token parameter variable">-rmr</span> <span class="token operator">&lt;</span>目录名<span class="token operator">&gt;</span> <span class="token comment">#删除目录</span></span>
<span class="line">    <span class="token parameter variable">-rm</span> <span class="token operator">&lt;</span>文件名<span class="token operator">&gt;</span> <span class="token comment">#删除文件</span></span>
<span class="line">    <span class="token parameter variable">-ls</span> <span class="token comment">#查看目录</span></span>
<span class="line">    <span class="token parameter variable">-cat</span> <span class="token operator">&lt;</span>文件名<span class="token operator">&gt;</span> <span class="token comment">#查看文件内容</span></span>
<span class="line">    <span class="token parameter variable">-put</span> <span class="token operator">&lt;</span>本地路径<span class="token operator">&gt;</span> <span class="token operator">&lt;</span>hdfs路径<span class="token operator">&gt;</span> <span class="token comment">#上传文件或目录到HDFS</span></span>
<span class="line">    <span class="token parameter variable">-copyToLocal</span> <span class="token operator">&lt;</span>hdfs路径<span class="token operator">&gt;</span> <span class="token operator">&lt;</span>本地路径<span class="token operator">&gt;</span> <span class="token comment">#从HDFS下载文件或目录</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h4 id="hdfs的配置" tabindex="-1"><a class="header-anchor" href="#hdfs的配置"><span>HDFS的配置</span></a></h4><h4 id="源码学习部分" tabindex="-1"><a class="header-anchor" href="#源码学习部分"><span>源码学习部分</span></a></h4><h3 id="mapreduce" tabindex="-1"><a class="header-anchor" href="#mapreduce"><span>MapReduce</span></a></h3><p>分为map和reduce两个步骤，处理输入的数据。</p><h4 id="mapreduce示例" tabindex="-1"><a class="header-anchor" href="#mapreduce示例"><span>MapReduce示例</span></a></h4><p>以下简单示例：用MapReduce统计每月出生最多人数。</p><p>第一阶段：初始输入参数（本地文本数据）</p><div class="language-txt line-numbers-mode" data-highlighter="prismjs" data-ext="txt"><pre><code class="language-txt"><span class="line">#出生年月|姓名|性别</span>
<span class="line">200001张三男</span>
<span class="line">200001李四女</span>
<span class="line">200101王五男</span>
<span class="line">200002梁六女</span>
<span class="line">040510王琪男</span>
<span class="line">...</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>第二阶段：map函数<br> 以年为key、姓名为value，提取文本数据；<br> 并将脏数据过滤掉（如040510王琪男这一行）。<br> 得到key-value数据数组</p><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text"><pre><code class="language-text"><span class="line">（200001,张三）,</span>
<span class="line">（200001,李四）,</span>
<span class="line">（200101,王五）,</span>
<span class="line">（200002,梁六）</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>第三阶段：MapReduce框架（shuffle）<br> 1.根据键进行分组；<br> 2.根据键进行排序；<br> 得到<code>Map&lt;key,List&lt;value&gt;&gt;</code></p><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text"><pre><code class="language-text"><span class="line">200001,[张三,李四]</span>
<span class="line">200002,梁六</span>
<span class="line">200101,王五</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>第四阶段：reduce函数<br> 取出分组中的人数。</p><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text"><pre><code class="language-text"><span class="line">200001,2</span>
<span class="line">200002,1</span>
<span class="line">200101,1</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>统计完成，整个流程结束。</p><p>以上展示了少量数据的情况下MapReduce的执行流程。然而在涉及到大量数据时，数据需要存储在分布式文件系统（HDFS）。</p><h4 id="概念" tabindex="-1"><a class="header-anchor" href="#概念"><span>概念</span></a></h4><p><strong>分片（split）</strong> Hadoop会将输入数据分成等长的分片，并为每个分片执行map任务。<br> 一个分片的默认大小是HDFS的一个块，64MB。</p><p><strong>任务</strong> MapReduce将任务分词若干小任务（task）：</p><ul><li>map任务：</li></ul><blockquote><p>一个分片对应一个map任务；<br> 多个map任务会在多个节点上执行；<br> 输出数据保存到文件系统而非HDFS</p></blockquote><ul><li>reduce任务:</li></ul><blockquote><p>将所有map任务的输出合并，作为输入；<br> 自定义执行的数量；输出到HDFS</p></blockquote><p><strong>分区（partition）</strong><br> 当reduce任务有多个时，会将map任务的输出按reduce进行分区（partition）；<br> 分区会按照用户指定的partition函数进行，每个reduce一个分区；<br> 默认的分区函数是按哈希函数来分区；<br> 每个分区有许多键，同一个键的键值对记录都会保存在同一分区；</p><p><strong>combiner</strong></p><p><strong>节点（物理机器）</strong><br> JobTracker节点：负责调度MR（MapReduce）任务，来协调整个系统上运行的任务<br> TaskTracker节点：负责运行MR任务，并将报告发送给JobTracker节点</p>`,71)]))}const d=s(i,[["render",p]]),o=JSON.parse('{"path":"/bigdata/Hadoop%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.html","title":"Hadoop学习笔记","lang":"en-US","frontmatter":{"title":"Hadoop学习笔记","sidebar":"heading"},"git":{"updatedTime":1750240340000,"contributors":[{"name":"dongyz8","username":"dongyz8","email":"dongyz8@gdii-yueyun.com","commits":3,"url":"https://github.com/dongyz8"}],"changelog":[{"hash":"ad8fc1a188d6829c38676e985c8e2097211af10d","time":1750240340000,"email":"dongyz8@gdii-yueyun.com","author":"dongyz8","message":"commit"},{"hash":"c97979e28496bfd6a49b98c58b764b3950f0de75","time":1739849131000,"email":"dongyz8@gdii-yueyun.com","author":"dongyz8","message":"commit"},{"hash":"7f927643cf84678c68bdb606a341073959279ad4","time":1734073104000,"email":"dongyz8@gdii-yueyun.com","author":"dongyz8","message":"commit"}]},"filePathRelative":"bigdata/Hadoop学习笔记.md"}');export{d as comp,o as data};
