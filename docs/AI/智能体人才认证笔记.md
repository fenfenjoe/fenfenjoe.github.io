# 智能体人才认证笔记

## 1.ChatGPT原理

### 1.1 大语言模型

大语言模型本质为**概率模型**。  

**流式输出**，按照概率，逐字生成回复内容。  

示例：
```  
你 （你：99%，我：1%，他：0%）
你好 （好：80%，是：18%，的：2%）
你好吗 （吗：99%，逗号：1%）
...
```

当前的大语言模型基本都基于**Transformer神经网络**构建。  
由2部分组成：
- Encoder:语言理解，语言编码
- Decoder:语言解码，语言生成

以Encoder为主要结构的语言模型：
- BERT（谷歌）、DistilBERT、XLM...

以Decoder为主要结构的语言模型：
- GPT（OpenAI）、CTRL...

以Encoder+Decoder为主要结构的语言模型：
- T5、BART、BigBird...


BERT模型不太出圈，原因是它基于编码器结构，理解识别语义能力突出，但生成能力较弱。  

而GPT模型在GPT-2，GPT-3阶段时，同样离用户很远，但凭借下面的优化，使其变得越来越友好：
- **指令微调**
- **基于人类反馈的强化学习（RLHF）**

### 1.2 ChatGPT

发展历程

| 年份    | 大语言模型        | 介绍                      |
|-------|--------------|-------------------------|
| 2013  | Word2Vec     | 单词编码                    |
| 2018  | GPT          | 单向语言模型（上文编码）            |
| 2018  | BERT         | 双向语言模型（上下文编码）           |
| 2019  | GPT-2        | 大模型（15亿参数）；大数据量（40G）    |
| 2020  | GPT-3        | 更大模型（1750亿参数）；大数据量（45T） |
| 2022  | InstructGPT  | 指令微调                    |
| 2022  | ChatGPT      | RLHF，基于人类反馈的强化学习        |


## 2. 大模型提示词工程基础

> 研究的是“如何能让大模型更容易输出自己想要的内容”

关键要素：
- 选择与设计
- 优化策略
- 效果评估

应用场景：
- 文章生成
- 机器翻译
- 问答系统
- 文章摘要
- 数据生成
- 数据打标
- 意图识别
- 本地知识库回答

基本要素：
- 指令（任务要明确）
- 上下文
- 输入数据
- 输出指示（限制输出的内容、格式）


使用技巧：
- 明确提出应该做什么，不应该做什么
- 提供输出的格式提示
- 提供输出的示例
- 增加角色或场景

