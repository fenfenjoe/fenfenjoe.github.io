---
title: 《深度学习》读书笔记
sidebar: 'heading'
sidebarDepth: 1
---

# 《深度学习》读书笔记 


作者： Ian Goodfellow,Yoshua Bengio,Aaron Courville

## 目录

### 第一部分 应用数学与机器学习基础
1. **引言**
    - 深度学习的历史与范畴
2. **线性代数**
    - 标量、向量、矩阵和张量的运算规则
3. **概率与信息论**
    - 概率分布、贝叶斯规则与信息熵
4. **数值计算**
    - 梯度优化与条件数分析
5. **机器学习基础**
    - 容量、过拟合与正则化概念

### 第二部分 深度网络：现代实践
6. **深度前馈网络**
    - 激活函数与反向传播算法
7. **深度学习中的正则化**
    - 参数范数惩罚与数据增强策略
8. **深度学习中的优化**
    - 自适应学习率算法（如Adam）
9. **卷积神经网络（CNN）**
    - 卷积核设计与图像特征提取
10. **循环神经网络（RNN）**
    - 长短期记忆网络（LSTM）与梯度消失问题
11. **实践方法论**
    - 超参数调优与性能评估指标

### 第三部分 深度学习研究前沿
12. **线性因子模型**
    - 概率PCA与独立成分分析
13. **自编码器**
    - 降维与生成式模型设计
14. **表示学习**
    - 无监督预训练与迁移学习框架
15. **结构化概率模型**
    - 图模型与马尔可夫随机场
16. **蒙特卡罗方法**
    - 马尔可夫链与Gibbs采样技术
17. **深度生成模型**
    - 生成对抗网络（GAN）与变分自编码器（VAE）


## 读书笔记

### 数学符号表示（P23）

### 第一部分-第五章：机器学习基础（P115）

- 我们将探讨如何使用额外的数据设置**超参数**。

-----

- 两种统计学的主要方法：**频率派估计**和**贝叶斯推断**。  

-----

- 大部分深度学习算法都是基于被称为**随机梯度下降**的算法求解的。

-----

- 我们将介绍如何组合不同的算法部分，例如优化算法、代价函数、模型和数据集，来建立一个机器学习算法。

-----

- 机器学习的一个简洁定义：对于某类**任务T**和**性能度量P**，一个计算机程序被认为可以从**经验E**中学习是指，通过经验E改进后，它在任务T上由性能度量P衡量的性能有所提升。

-----

- 我自己总结的机器学习解决过程：**样本->特征的集合->特征向量化->输入->机器学习算法->输出**

-----

**【任务T】** 机器学习能解决很多类型的任务：
- 分类（比如对象识别、人脸识别）
- 输入缺失分类（一些输入丢失下做分类）
- 回归（预测未来值）
- 转录（图生文、音生文）
- 翻译（语言A转换成语言B）
- 结构化输出
- 异常检测（信用卡欺诈检测）
- 合成和采样（视频音频创作）
- 缺失值填补
- 去噪
- 密度估计或概率质量函数估计


**【性能度量P】** 为了评估机器学习算法的能力，必须要设计其**性能的定量度量**，一般有：
- 准确率
- 错误率
- 测试集

**【经验E】** 根据学习过程中的不同经验，机器学习算法可以大致分类为 **无监督（unsupervised）算法**和 **监督（supervised）算法**

-----

- 传统地，人们将回归、分类或者结构化输出问题称为监督学习。支持其他任务的密度估计通常被称为无监督学习。

-----

- **欧几里得距离**：也称为**欧几里得范数**，用 $||x||_2$表示。
  - 几何意义是：在多维空间中，L2范数是点x到原点的直线距离，对应勾股定理的推广

-----

**【正规方程】**：正规方程，又被称为**解析解**，是求解线性回归模型**最优参数**的方法。  
对比另一种求解方法**梯度下降**，正规方程更适合用于特征数小的模型。

-----

**【损失函数】**：又叫代价函数（Cost Function），通常用$J(\theta)$表示。是模型训练的核心工具，用于‌量化预测结果与真实值的偏差‌。
- 目标：通过调整模型参数，**最小化损失函数‌**，使预测值尽可能接近真实值。
- 常见损失函数：
  - 回归问题：MSE、MAE（平均绝对误差）
  - 分类问题：交叉熵（Cross-Entropy）、Hinge Loss

-----

    
一个机器学习示例：线性回归模型
- 【定义模型】定义一个线性回归模型$y = \theta^T X+\epsilon$，其中$\theta$是参数向量，X是特征向量，$\epsilon$是误差。
  - 如果有5个特征，展开之后就是 $y=\theta_0 + \theta_1 x_1 + \theta_2 x_2 + \theta_3 x_3 + \theta_4 x_4 + \theta_5 x_5$，其中$\theta_0$是**截距项**
- 【获取训练集】**获取样本作为训练集。** 样本数应达到特征数的10倍，这样能提升泛化能力。获取后将样本以**向量**的形式表示，组成特征向量X。
- 【最小化损失函数，获得最优参数】以**均方误差（MSE）** 作为损失函数，求解损失函数最小的时候的参数向量$\theta$
  - 均方误差函数：$J(\theta)=\frac{1}{2m}||X\theta-y||^2$
  - 损失函数展开、对$\theta$求导并令导数为0，化简得**正规方程**：$\theta=(X^T X)^{-1}X^T y$
  - 通过正规方程，求得参数向量$\theta$。同时获得一个线性回归模型。
- 【获取测试集】取部分样本作为**测试集**，测试集负责用来评估模型性能。
- 【度量性能】同样，通过计算测试集上的 **均方误差（MSE）** 来度量模型性能。

-----


- 机器学习的主要挑战是我们的算法必须**能够在先前未观测的新输入上表现良好**（测试误差低），而不只是在训练集上表现良好（训练误差低）。
- 算法在先前未观测到的输入上表现良好的能力被称为**泛化（generalization）**

-----

训练模型会遇到的挑战：
- **欠拟合**：模型不能在训练集上获得足够低的误差
- **过拟合**：训练误差和测试误差之间的差距太大

-----

- 通过调整模型的**容量**，我们可以控制模型是否偏向于**过拟合**或者**欠拟合**。  
- $y =b + \theta x$，一次多项式，容量最小，容易欠拟合，样本点落不到直线上。
- $y =b + \theta_1 x_1 + \theta_2 x_2^2$，增加一个二次项，容量增加。
- $y =b + \theta_1 x_1 + \theta_2 x_2^2 + ... \theta_9 x_9^9$，9次多项式，容量更大，但容易过拟合。

-----

**【正则化项】**：正则化项是添加到损失函数中的额外项，用于惩罚模型复杂度，防止过拟合。通过限制参数的大小，促使模型学习更简单、泛化性更强的规律。   
常见有：
- L1正则化（Lasso）：正则化项 = $\lambda \sum_{j=1}^{n} |\theta_j|$
- L2正则化（Ridge）：正则化项 = $\lambda \sum_{j=1}^{n} \theta_j^2$

其中的$\lambda$是**正则化系数**，用来控制正则化强度。

添加后的完整代价函数： $J(\theta)=原始损失项+\lambda 正则化项$

-----

**【超参数】**：是模型训练前需要**手动设定的参数**，它们控制模型的训练过程、结构或优化行为，但‌不通过数据直接学习‌。  
常见超参数有：
- 学习率
- 正则化系数
- 网络层数
- 决策树深度
- 批量大小
- ...

-----

**【验证集】**：从训练数据中挑选一部分作为验证集，用于**挑选超参数**。

-----

**【点估计】**：用样本数据统计出的单一数值作为总体的估计值。  
如：
- 用样本均值估计总体均值
- 用样本方差估计总体方差
优点：计算简单；缺点：无法反映估计的不确定性，需结合**区间估计**或**置信区间**


**【偏差】**：估计值的期望值$E(\theta)$与真实值$\theta$的差距  
- 公式：$Bias(\theta) = E(\theta) - \theta$
- **无偏估计**：$E(\theta) = \theta$ 
  - **样本均值**估计**总体均值**
  - **修正样本方差**估计**总体方差**
  - **OLS回归系数**估计**真实参数**
  - **样本比例**估计**总体比例**
- **有偏估计**：$E(\theta) != \theta$
  - **未修正样本方差**估计**总体方差**
  - **极大似然估计（MLE）的方差**估计**正态分布**
  - **岭回归（Ridge Regression）系数**估计
  - LASSO回归系数估计
  - 样本中位数估计总体均值
- 例子：用线性模型拟合非线性数据，导致高偏差（预测值与真实值存在系统性偏离）。


**【方差】**：估计值与真实值差距的波动程度，反映‌对数据扰动的敏感性‌。
- 公式：$Var(\theta) = E((\theta - E(\theta))^2)$
- 模型若过于复杂，则方差容易过大。

核心矛盾：
- 降低偏差‌：需增加模型复杂度（如使用深度网络），但可能导致高方差。
- ‌降低方差‌：需简化模型（如线性回归），但可能导致高偏差。

可用**靶心图**描述模型的方差与偏差：
- **低偏差低方差**（弹孔密集且靠近靶心）
- **高偏差高方差**（弹孔分散且偏离靶心）。

示例：  
案例1：均值估计 vs. 中位数估计‌
- 均值‌：无偏但受异常值影响大（高方差）。
- ‌中位数‌：对异常值鲁棒（低方差），但对对称分布可能有偏差。

‌案例2：岭回归（Ridge Regression）‌
- 通过添加L2正则化，牺牲少量偏差以大幅降低方差，提升泛化性。

-----




看到P142
